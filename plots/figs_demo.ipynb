{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# TODO remove\n",
    "import sys,os\n",
    "sys.path.append(os.path.expanduser('~/imodels'))\n",
    "sys.path.append(os.path.expanduser('~/dtreeviz'))\n",
    "\n",
    "########################################################\n",
    "# python\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "norm = scipy.stats.norm\n",
    "import bisect\n",
    "\n",
    "########################################################\n",
    "# figs (imodels), xgboost, sklearn\n",
    "import imodels\n",
    "from imodels import FIGSClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "########################################################\n",
    "# dtreeviz\n",
    "# must follow the package README to properly install all dependencies!\n",
    "\n",
    "from dtreeviz import trees\n",
    "from dtreeviz.models.sklearn_decision_trees import ShadowSKDTree\n",
    "from imodels.tree.viz_utils import extract_sklearn_tree_from_figs\n",
    "\n",
    "from wand.image import Image\n",
    "from svglib.svglib import svg2rlg\n",
    "from reportlab.graphics import renderPDF\n",
    "\n",
    "########################################################\n",
    "# skompiler\n",
    "from skompiler import skompile\n",
    "\n",
    "########################################################\n",
    "# plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.transforms\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.')\n",
    "\n",
    "########################################################\n",
    "# set global rnd_seed for reproducibility\n",
    "rnd_seed = 42\n",
    "np.random.seed(rnd_seed)\n",
    "\n",
    "datasets = ['train', 'holdout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting import * # load plotting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inline=True # plot inline or to pdf\n",
    "output = './figs_demo_output' # output dir\n",
    "os.makedirs(output, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dtreeviz(viz, m_path, fname, tag=''):\n",
    "    os.makedirs(m_path, exist_ok=True)\n",
    "    full_path = f'{m_path}/{fname}{tag}'\n",
    "\n",
    "    # svg\n",
    "    viz.save(f'{full_path}.svg')\n",
    "\n",
    "    # pdf via svglib\n",
    "    renderPDF.drawToFile(svg2rlg(f'{full_path}.svg'), f'{full_path}.pdf')\n",
    "\n",
    "    # png via wand / ImageMagick\n",
    "    img = Image(filename=f'{full_path}.svg', resolution=500)\n",
    "    img.format = 'png'\n",
    "    img.save(filename=f'{full_path}.png')\n",
    "\n",
    "    # clean up graphviz dot file (no extension)\n",
    "    os.remove(full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "# Generate Random Data\n",
    "Include additive structure that FIGS does well on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_params_all = {'n_samples': int(1e5), 'n_classes': 2, 'shuffle': False, 'shift': 0.0, 'scale': 1.0, 'hypercube': True}\n",
    "\n",
    "mc_params = [\n",
    "    {'n_features': 20, 'n_informative': 6, 'n_redundant': 4, 'n_repeated': 0,\n",
    "     'n_clusters_per_class': 5, 'weights': [0.5], 'flip_y': 0.05, 'class_sep': 0.9},\n",
    "    {'n_features': 10, 'n_informative': 4, 'n_redundant': 2, 'n_repeated': 0,\n",
    "     'n_clusters_per_class': 2, 'weights': [0.7], 'flip_y': 0.1, 'class_sep': 0.9},\n",
    "    {'n_features': 5, 'n_informative': 2, 'n_redundant': 2, 'n_repeated': 0,\n",
    "     'n_clusters_per_class': 2, 'weights': [0.6], 'flip_y': 0.04, 'class_sep': 0.9},\n",
    "]\n",
    "\n",
    "X = None\n",
    "y = None\n",
    "feat_names = []\n",
    "\n",
    "for i_mc_param, mc_param in enumerate(mc_params):\n",
    "    param = {**mc_params_all, **mc_param, 'random_state': rnd_seed+i_mc_param}\n",
    "    X_i, y_i = make_classification(**param)\n",
    "    if X is None:\n",
    "        X = X_i\n",
    "    else:\n",
    "        X = np.concatenate([X, X_i], axis=1)\n",
    "    if y is None:\n",
    "        y = y_i\n",
    "    else:\n",
    "        y = np.logical_and(y, y_i).astype(int)\n",
    "    feat_names += [f'x_{i_mc_param}_{_}' for _ in range(X_i.shape[1])]\n",
    "    del X_i; del y_i;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Train, Validation, and Holdout Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainVal, X_holdout, y_trainVal, y_holdout = train_test_split(X, y, test_size=0.15, random_state=rnd_seed, stratify=y)\n",
    "del X; del y;\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainVal, y_trainVal, test_size=0.2, random_state=rnd_seed, stratify=y_trainVal)\n",
    "# del X_trainVal; del y_trainVal;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "# FIGS\n",
    "Note we are not using early stopping with FIGS, so use `X_trainVal` during training to take advantage of all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_figs = FIGSClassifier(max_rules=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_figs_start = time.time()\n",
    "model_figs.fit(X_trainVal, y_trainVal, feature_names=feat_names);\n",
    "time_figs_end = time.time()\n",
    "print(f'FIGS ran in {time_figs_end-time_figs_start:.0f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_splits_figs(model):\n",
    "    splits = []\n",
    "    for tree_ in model.trees_:\n",
    "        node_counter = iter(range(1, int(1e06)))\n",
    "        def _count_node(node):\n",
    "            if node.left is None:\n",
    "                return\n",
    "            node_id=next(node_counter)\n",
    "            _count_node(node.left)\n",
    "            _count_node(node.right)\n",
    "\n",
    "        _count_node(tree_)\n",
    "        splits.append(next(node_counter)-1)\n",
    "    return sum(splits)\n",
    "\n",
    "n_splits_figs = count_splits_figs(model_figs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'FIGS used {len(model_figs.trees_)} trees and {n_splits_figs:,} splits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_figs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_figs.print_tree(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_figs.plot(fig_size=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_default = {'max_depth': 6, 'learning_rate': 0.3, 'gamma': 0.0, 'reg_alpha': 0.0, 'reg_lambda': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_setup_params = {\n",
    "    'max_num_boost_rounds': 500, # maximum number of boosting rounds to run / trees to create\n",
    "    'xgb_objective': 'binary:logistic', # objective function for binary classification\n",
    "    'xgb_verbosity': 0, #  The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
    "    'xgb_n_jobs': -1, # Number of parallel threads used to run XGBoost. -1 makes use of all cores in your system\n",
    "    'eval_metric': 'auc', # evaluation metric for early stopping\n",
    "    'early_stopping_rounds': 10, # must see improvement over last num_early_stopping_rounds or will halt\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_fit_params = {\n",
    "    'eval_set': [(X_val, y_val)], # data sets to use for early stopping evaluation\n",
    "    'verbose': False, # even more verbosity control\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgboost = xgb.XGBClassifier(n_estimators=fixed_setup_params['max_num_boost_rounds'],\n",
    "                                  objective=fixed_setup_params['xgb_objective'],\n",
    "                                  verbosity=fixed_setup_params['xgb_verbosity'],\n",
    "                                  eval_metric=fixed_setup_params['eval_metric'],\n",
    "                                  early_stopping_rounds=fixed_setup_params['early_stopping_rounds'],\n",
    "                                  random_state=rnd_seed+3, **params_default, use_label_encoder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_xgboost_start = time.time()\n",
    "model_xgboost.fit(X_train, y_train, **fixed_fit_params);\n",
    "time_xgboost_end = time.time()\n",
    "print(f'XGBoost ran in {time_xgboost_end-time_xgboost_start:.0f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits_xgboost = sum([tree.count('\"split\"') for tree in model_xgboost.get_booster().get_dump(dump_format='json')[0:model_xgboost.best_ntree_limit]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'XGBoost used {model_xgboost.best_ntree_limit} trees and {n_splits_xgboost:,} splits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'XGBoost used {n_splits_xgboost:,} splits vs FIGS {n_splits_figs:,}')\n",
    "print(f'That is {n_splits_xgboost-n_splits_figs:,}, or {(n_splits_xgboost-n_splits_figs)/n_splits_figs:,.0%}, more splits!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_metrics(model, X_train, y_train, X_holdout, y_holdout, feature_names, do_permutation_importance=True, print_classification_report=False):\n",
    "    model_metrics = {}\n",
    "    dfp_importance = pd.DataFrame({'feature': feature_names})\n",
    "    dfp_importance['icolX'] = dfp_importance.index\n",
    "\n",
    "    for dataset in datasets[::-1]:\n",
    "        if dataset == 'holdout':\n",
    "            X = X_holdout\n",
    "            y = y_holdout\n",
    "        elif dataset == 'train':\n",
    "            X = X_train\n",
    "            y = y_train\n",
    "        y_pred = model.predict(X)\n",
    "        # only want positive class prob\n",
    "        try:\n",
    "            # use best_iteration for XGBoost\n",
    "            y_pred_prob = model.predict_proba(X, iteration_range=(0, model.best_iteration+1))[:, 1]\n",
    "        except:\n",
    "            y_pred_prob = model.predict_proba(X)[:, 1]\n",
    "\n",
    "        model_metrics[dataset] = {}\n",
    "        model_metrics[dataset]['accuracy_score'] = metrics.accuracy_score(y, y_pred)\n",
    "        model_metrics[dataset]['precision_score'] = metrics.precision_score(y, y_pred, zero_division=0) # zero_division=0 hides divide by zero warnings that come up with LR doesn't converge\n",
    "        model_metrics[dataset]['recall_score'] = metrics.recall_score(y, y_pred)\n",
    "        model_metrics[dataset]['f1_score'] = metrics.f1_score(y, y_pred)\n",
    "        model_metrics[dataset]['roc_auc_score'] = metrics.roc_auc_score(y, y_pred_prob)\n",
    "        model_metrics[dataset]['average_precision_score'] = metrics.average_precision_score(y, y_pred_prob) # PR ROC AUC\n",
    "        model_metrics[dataset]['log_loss'] = metrics.log_loss(y, y_pred)\n",
    "        model_metrics[dataset]['cohen_kappa_score'] = metrics.cohen_kappa_score(y, y_pred)\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "        CM = metrics.confusion_matrix(y, y_pred)\n",
    "        model_metrics[dataset]['confusion_matrix'] = CM\n",
    "        model_metrics[dataset]['TN'] = CM[0][0]\n",
    "        model_metrics[dataset]['FP'] = CM[0][1]\n",
    "        model_metrics[dataset]['FN'] = CM[1][0]\n",
    "        model_metrics[dataset]['TP'] = CM[1][1]\n",
    "        model_metrics[dataset]['TNR'] = CM[0][0] / (CM[0][0] + CM[0][1]) # TN / (TN + FP)\n",
    "        model_metrics[dataset]['NPV'] = CM[0][0] / (CM[0][0] + CM[1][0]) # TN / (TN + FN)\n",
    "\n",
    "        model_metrics[dataset]['pop_PPV'] = len(np.where(y == 1)[0]) / len(y) # P / (P + N)\n",
    "\n",
    "        # model_metrics[dataset]['dfp_y'] = pd.DataFrame( {'y': y, 'y_pred_prob': y_pred_prob, 'y_pred': y_pred} )\n",
    "\n",
    "        # for LR models\n",
    "        # model_converged = (model.n_iter_ < model.max_iter)[0]\n",
    "\n",
    "        if print_classification_report:\n",
    "            print(f'For {dataset}:')\n",
    "            print(metrics.classification_report(y, y_pred))\n",
    "\n",
    "        # ROC Curves\n",
    "        def get_n_predicted_positive_vs_thr(y_pred_prob, thr):\n",
    "            y_pred_prob_sorted = sorted(y_pred_prob)\n",
    "            return [len(y_pred_prob_sorted) - bisect.bisect_left(y_pred_prob_sorted, _thr) for _thr in thr]\n",
    "\n",
    "        fpr, tpr, thr_of_fpr_tpr = roc_curve(y, y_pred_prob)\n",
    "        n_predicted_positive_vs_thr_of_fpr_tpr = get_n_predicted_positive_vs_thr(y_pred_prob, thr_of_fpr_tpr)\n",
    "        dfp_eval_fpr_tpr = pd.DataFrame({'fpr': fpr, 'tpr': tpr, 'thr': thr_of_fpr_tpr, 'n_predicted_positive': n_predicted_positive_vs_thr_of_fpr_tpr}).sort_values(by='thr').reset_index(drop=True)\n",
    "\n",
    "        precision, recall, thr_of_precision_recall = precision_recall_curve(y, y_pred_prob)\n",
    "        thr_of_precision_recall = np.insert(thr_of_precision_recall, 0, [0])\n",
    "        n_predicted_positive_vs_thr_of_precision_recall = get_n_predicted_positive_vs_thr(y_pred_prob, thr_of_precision_recall)\n",
    "        dfp_eval_precision_recall = pd.DataFrame({'precision': precision, 'recall': recall, 'thr': thr_of_precision_recall, 'n_predicted_positive': n_predicted_positive_vs_thr_of_precision_recall})\n",
    "        dfp_eval_precision_recall['f1'] = 2*(dfp_eval_precision_recall['precision'] * dfp_eval_precision_recall['recall']) / (dfp_eval_precision_recall['precision'] + dfp_eval_precision_recall['recall'])\n",
    "\n",
    "        model_metrics[dataset]['dfp_eval_fpr_tpr'] = dfp_eval_fpr_tpr\n",
    "        model_metrics[dataset]['dfp_eval_precision_recall'] = dfp_eval_precision_recall\n",
    "\n",
    "        if do_permutation_importance:\n",
    "            # print('Start do_permutation_importance func')\n",
    "            # Permutation feature importance\n",
    "            # slow for thousands of features!\n",
    "            # https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html\n",
    "            # https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance\n",
    "            # https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py\n",
    "            # https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py\n",
    "            _permutation_importance = permutation_importance(model, X, y, n_repeats=10, random_state=42, n_jobs=-1, scoring='roc_auc')\n",
    "            # print('End permutation_importance func')\n",
    "\n",
    "            importance_permutation_mean = _permutation_importance['importances_mean']\n",
    "            importance_permutation_std = _permutation_importance['importances_std']\n",
    "            dfp_importance_permutation = pd.DataFrame({f'importance_permutation_{dataset}_mean': importance_permutation_mean, f'importance_permutation_{dataset}_std': importance_permutation_std})\n",
    "            dfp_importance_permutation['icolX'] = dfp_importance_permutation.index\n",
    "            dfp_importance_permutation[f'importance_permutation_{dataset}_pct'] = dfp_importance_permutation[f'importance_permutation_{dataset}_mean'].rank(pct=True)\n",
    "            dfp_importance = pd.merge(dfp_importance, dfp_importance_permutation, on='icolX', how='left')\n",
    "\n",
    "    # for LR models\n",
    "    # dfp_coef = pd.DataFrame({'coefficients': model.coef_[0]})\n",
    "    # dfp_coef['abs_coeff'] = dfp_coef['coefficients'].abs()\n",
    "    # dfp_coef['icolX'] = dfp_coef.index\n",
    "    # dfp_importance = pd.merge(dfp_importance, dfp_coef, on='icolX', how='left')\n",
    "\n",
    "    # TODO\n",
    "    # Gini impurity importance - a mean decrease in impurity (MDI) importance (both RF and BDT)\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_\n",
    "    # importances_gini = model.feature_importances_\n",
    "    # estimators = model.estimators_\n",
    "    # importances_gini_std = np.std([tree.feature_importances_ for tree in estimators], axis=0)\n",
    "    # dfp_importance_gini = pd.DataFrame({'importance_gini': importances_gini, 'importance_gini_std': importances_gini_std})\n",
    "    # dfp_importance_gini['icolX'] = dfp_importance_gini.index\n",
    "    # dfp_importance_gini['importance_gini_pct'] = dfp_importance_gini['importance_gini'].rank(pct=True)\n",
    "    # dfp_importance = pd.merge(dfp_importance, dfp_importance_gini, on='icolX', how='left')\n",
    "\n",
    "    target_cols_importance = [\n",
    "        'feature',\n",
    "        # 'coefficients',\n",
    "        'importance_permutation_holdout_mean',\n",
    "        'importance_permutation_holdout_std',\n",
    "        'importance_permutation_holdout_pct',\n",
    "        'importance_permutation_train_mean',\n",
    "        'importance_permutation_train_std',\n",
    "        'importance_permutation_train_pct',\n",
    "        # 'importance_gini',\n",
    "        # 'importance_gini_std',\n",
    "        # 'importance_gini_pct',\n",
    "        'icolX',\n",
    "        # 'abs_coeff',\n",
    "    ]\n",
    "    _cols = [_col for _col in target_cols_importance if _col in dfp_importance.columns] + [_col for _col in dfp_importance.columns if _col not in target_cols_importance]\n",
    "    dfp_importance = dfp_importance[_cols]\n",
    "    if 'importance_permutation_holdout_mean' in dfp_importance.columns:\n",
    "        sort_col = 'importance_permutation_holdout_mean'\n",
    "    elif 'importance_gini' in dfp_importance.columns:\n",
    "        sort_col = 'importance_gini'\n",
    "    else:\n",
    "        sort_col = 'icolX'\n",
    "    dfp_importance = dfp_importance.sort_values(by=sort_col, ascending=False).reset_index(drop=True)\n",
    "\n",
    "    dfp_importance = dfp_importance.drop(['icolX'], axis=1)\n",
    "    # if 'abs_coeff' in dfp_importance.columns:\n",
    "    #     dfp_importance = dfp_importance.drop(['abs_coeff'], axis=1)\n",
    "\n",
    "    model_metrics['dfp_importance'] = dfp_importance\n",
    "\n",
    "    return model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics_figs = classifier_metrics(model_figs, X_trainVal, y_trainVal, X_holdout, y_holdout, feat_names)\n",
    "model_metrics_xgboost = classifier_metrics(model_xgboost, X_train, y_train, X_holdout, y_holdout, feat_names)\n",
    "\n",
    "metric_rows = []\n",
    "for model, name, model_metrics in [(model_figs, 'FIGS', model_metrics_figs), (model_xgboost, 'XGBoost', model_metrics_xgboost)]:\n",
    "    for dataset in datasets[::-1]:\n",
    "        dataset_metrics = {'model': name, 'dataset': dataset}\n",
    "        for k,v in model_metrics[dataset].items():\n",
    "            if k not in ['confusion_matrix', 'dfp_eval_fpr_tpr', 'dfp_eval_precision_recall', 'dfp_y']:\n",
    "                dataset_metrics[k] = v\n",
    "        metric_rows.append(dataset_metrics)\n",
    "\n",
    "dfp_metrics = pd.DataFrame(metric_rows)\n",
    "dfp_metrics = dfp_metrics.sort_values(by=['model', 'dataset'], ascending=[True, True]).reset_index(drop=True)\n",
    "display(dfp_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FIGS Feature Importances')\n",
    "_dfp = model_metrics_figs['dfp_importance']\n",
    "display(_dfp.loc[0 < _dfp['importance_permutation_holdout_mean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('XGBoost Feature Importances')\n",
    "_dfp = model_metrics_xgboost['dfp_importance']\n",
    "display(_dfp.loc[0 < _dfp['importance_permutation_holdout_mean']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_for_roc_dict = {\n",
    "#     {**{'name': 'FIGS_train', 'nname': 'FIGS (Train)', 'c': 'C2', 'ls': '-'}, **roc_},\n",
    "#     {**{'name': 'XGBoost', 'nname': 'XGBoost', 'c': 'black', 'ls': '--'}, **roc_},\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Standard TPR vs FPR ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_rocs(models_for_roc, m_path=f'{output}/roc_curves', rndGuess=False, inverse_log=False, inline=inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Precision vs Recall ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_rocs(models_for_roc, m_path=f'{output}/roc_curves', rndGuess=False, inverse_log=False, precision_recall=True,\n",
    "#     pop_PPV=model_metrics[dataset]['pop_PPV'], y_axis_params={'min': -0.05}, inline=inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "# Tree Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_figs_0 = extract_sklearn_tree_from_figs(model_figs, tree_num=0, n_classes=2)\n",
    "sk_figs_0 = ShadowSKDTree(dt_figs_0, X_train, y_train, feat_names, 'y', [0, 1])\n",
    "\n",
    "dt_figs_1 = extract_sklearn_tree_from_figs(model_figs, tree_num=1, n_classes=2)\n",
    "sk_figs_1 = ShadowSKDTree(dt_figs_1, X_train, y_train, feat_names, 'y', [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtreeviz.colors import color_blind_friendly_colors # mpl_colors # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = trees.dtreeviz(sk_figs_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dtreeviz(viz, output, 'dtreeviz_figs_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = trees.dtreeviz(sk_figs_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = trees.ctreeviz_leaf_samples(sk_figs_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = trees.ctreeviz_leaf_samples(sk_figs_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_figs_0 = skompile(dt_figs_0.predict_proba, feat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expr_figs_0.to('sqlalchemy/sqlite', component=1, assign_to='tree_0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(expr_figs_0.to('python/code'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_figs_1 = skompile(dt_figs_1.predict_proba, feat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expr_figs_1.to('sqlalchemy/sqlite', component=1, assign_to='tree_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(expr_figs_1.to('python/code'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XGBoost\n",
    "Tree 0 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
