%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Time Series Analysis}
\label{additional:time_series}
% https://www.youtube.com/playlist?list=PLvcbYUQ5t0UHOLnBzl46_Q6QKtFgfMGc3

Time series analysis is the study of time-dependent data such as price over time.
Similar to regression models we aim to fit a model
to the data in order to make future predictions.
However, instead of interpolating within a range of previously observed data points,
time series aims to extrapolate from our current data into the future,
as we are not interested in making predictions about past dates.
However, as we extrapolate further and further away from observed data
the uncertainty of our predictions will naturally grow,
making time series a harder problem to address than normal regression.
There are a variety of potential models to choose from,
each with their own assumptions and use cases.
The problem can be attacked from two general directions,
frequency-domain methods and time-domain methods,
but we shall focus primarily on the time-domain here.

\subsubsection{Lag Notation}
\label{additional:time_series:L}

When working with time series data we often index time steps as $t$, $t-1$, $t-2$,\,\ldots\,
where $t$ is the current time.
The value of the variable in question, price, quantity, \etc,
at time $t$ is then $y_{t}$, with $y_{t-1}$, $y_{t-2}$,\,\ldots\, at subsequent times.
Instead of writing subscripts, sometimes it is more convenient
to switch to a polynomial based notation using
the lag operator $L$, also known as the backshift operator $B$ in some texts:

\begin{equation}\label{eq:time_series:L}
L y_{t} = y_{t-1},\quad L^{2} y_{t} = y_{t-2},\,\ldots \quad L^{k} y_{t} = y_{t-k}.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Correlation in Time Series}
\label{additional:time_series:correlation}

In time series analysis $y_{t}$ can depend, to varying degree,
on the previous values $L^{k} y_{t},\, \forall k$.
There is a direct component to this dependence,
$y_{t} \sim f\left(y_{t-1}\right)$,
but also an indirect or recursive component,
$y_{t} \sim f\left(y_{t-1}\right) \sim f\left(g\left(y_{t-2}\right)\right)$.

\subsubsection{Auto-Correlation Function (ACF)}
\label{additional:time_series:ACF}

Neglecting the multiple components of the correlation of $y_{t}$ with prior times,
we can commute the overall auto-correlation function (ACF) of $y_{t}$
with itself at some amount of lag $k$ as

\begin{equation}\label{eq:time_series:ACF}
\text{ACF}\left(y,k\right) = \corr{y_{t}}{y_{t-k}},
\end{equation}

\noindent using the regular Pearson correlation coefficient \cref{eq:stats:corr}.
The auto-correlation shows the total correlation and is simple to compute,
but mixes the direct and indirect effects of prior values together.

\subsubsection{Partial Auto-Correlation Function (PACF)}
\label{additional:time_series:PACF}

In contrast, partial auto-correlation function
aims to isolate the direct influence of $y_{t-k}$ on $y_{t}$ for some lag $k$.
We can get at this component by fitting $y_{t}$ as a linear function of $L^{n} y_{t}$,

\begin{equation}\label{eq:time_series:PACF}
y_{t} = \sum_{i=1}^{k} \phi_{k,i}\, y_{t-i} + \epsilon_{k},
\end{equation}

\noindent and identifying $\text{PACF}\left(y,k\right) = \phi_{k,k}$
as the partial auto-correlation function for $k$.
Note that for each $k$, we need to do a new fit of \cref{eq:time_series:PACF},
and after fitting should check that $\phi_{k,k}$ is statistically significant,
\ie we reject $H_{0}$ that $\phi_{k,k}=0$, by looking at the coefficient's confidence interval from the fit.
We can generate plots of ACF and PACF vs $k$ to explore a time series
with the \texttt{plot\_acf} and \texttt{plot\_pacf}
\href{https://www.statsmodels.org/stable/graphics.html}{functions} in \texttt{statsmodels.graphics.tsaplots}.

% TODo add example plots, talk about how to interpret, ie what model's they suggest we try

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Stationarity}
\label{additional:time_series:stationarity}

Stationarity is an important assumption for many parametric time series models,
which combines three properties:

\begin{enumerate}[noitemsep]
\item The mean $\expval{y}$ of the time series is constant, \ie the average is not moving.\label{item:time_series:stationarity:constant_mean}
\item The variance $\variance{y}$ of the time series is constant.\label{item:time_series:stationarity:constant_var}
\item The time series does not exhibit seasonality, \ie no periodic fluctuations in $y\left(t\right)$.
\label{item:time_series:stationarity:seasonality}
\end{enumerate}

If the mean or variance is growing with time
we can look at the first difference, $d_{t} = y_{t} -y_{t-1}$,
and see if the time series of $d_{t}$ is stationary.
If $d$ is stationary and we can build a model for it, we will
need to transform back to $y_{t} = d_{t} + y_{t-1}$ when making predictions.
Similarly, we can try to remove seasonality
by subtracting a deterministic time-dependent function.

Simply looking at the graph of a time series can give us
a good intuition on if it is stationary or not.
However, we can formally test for stationarity
with the augmented Dickey--Fuller test (ADF) of \cref{additional:time_series:ADF}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{White Noise}
\label{additional:time_series:white_noise}

In the context of time series analysis, white noise is random fluctuations which
have mean zero, $\expval{y} = 0$,
constant variance with respect to time, $\partial_{t} \variance{y} = 0$,
and no correlation between lags, $\corr{y_{t}}{y_{t-k}} = 0,\,\, \forall k$.
Ideally after fitting a time series, we will capture all of the signal
in the model and the resulting error term $\epsilon$ will only contain white noise.
We can test for white noise by looking at the moving average and ACF plot,
or more formally with the Ljung--Box test of \cref{additional:time_series:ljung_box}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Autoregressive (AR) Models}
\label{additional:time_series:AR}
Building on the PACF, we can construct an autoregressive (AR) model $\text{AR}\left(p\right)$
from multiple lag components of the time series:

\begin{equation}\label{eq:time_series:AR}
\hat{y}_{t} = \phi_{0} + \sum_{i=1}^{p} \phi_{i}\, y_{t-i} + \epsilon_{t}.
\end{equation}

Note that we should look at the PACF plot before fitting \cref{eq:time_series:AR}
to determine which lags to include,
\ie set $p$ and decide which lags to skip\footnote{It may not be possible
to specify individual lags to include in some software packages,
but excluding non-significant coefficients and reducing model complexity
is still a good idea if possible.} by setting $\phi_{i} = 0$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Moving Average (MA) Models}
\label{additional:time_series:MA}

Instead of looking at the autocorrelation $y_{y-i}$,
we can also build a time series model around
correcting for prior errors in the prediction,
\ie fitting to the moving average (MA).
The $\text{MA}\left(q\right)$ model
is then:

\begin{subequations}\label{eq:time_series:MA}
\begin{align}
\hat{y}_{t} &= \theta_{0} + \sum_{i=1}^{q} \theta_{i}\, \epsilon_{t-i} + \epsilon_{t}, \label{eq:time_series:MA_y} \\
\epsilon_{t} &= y_{t} - \hat{y}_{t}. \label{eq:time_series:MA_epsilon}
\end{align}
\end{subequations}

Note that a $\text{MA}\left(q\right)$ model will fluctuate around the initial average $\theta_{0}$,
and can only predict $q$ steps into the future before constantly returning $\theta_{0}$.
We can select $q$ by looking at the ACF plot
and identifying the last non-zero lag.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Invertibility of Time Series}
\label{additional:time_series:invert}
% https://youtu.be/QU_VNu3rJKY
% https://youtu.be/q0vz7dGlZL0

One interesting property of AR and MA time series models is their invertibility:

\begin{subequations}\label{eq:time_series:invert}
\begin{align}
\text{MA}\left(1\right) &= \text{AR}\left(\infty\right), \label{eq:time_series:invert:MA_1} \\
\text{AR}\left(1\right) &= \text{MA}\left(\infty\right). \label{eq:time_series:invert:AR_1}
\end{align}
\end{subequations}

For a proof of \cref{eq:time_series:invert:MA_1}, see \cref{eq:time_series:invert_proof}.
Letting $\theta = \phi < 0$ for convenience and
assuming\footnote{Here we are assuming the time series is stationary, \ie no unit roots $\abs{\phi} < 1$.} $\abs{\phi} < 1$
we can construct the time series $C_{t}$,

\begin{subequations}\label{eq:time_series:invert_proof}
\begin{align}
C_{t} \equiv \text{MA}\left(1\right) - \theta_{0} &= -\phi\, \epsilon_{t-1} + \epsilon_{t} = \left(1 - \phi L \right) \epsilon_{t}, \label{eq:time_series:invert_proof:a} \\
\implies \epsilon_{t} &= \frac{C_{t}}{1 - \phi L}, \label{eq:time_series:invert_proof:b} \\
&= \left(\sum_{i=0}^{\infty} \phi^{i} L^{i} \right) C_{t} = \sum_{i=0}^{\infty} \phi^{i} C_{t-i}, \label{eq:time_series:invert_proof:c} \\
\implies C_{t} &= \sum_{i=1}^{\infty} -\phi^{i} C_{t-i} + \epsilon_{t} = \text{AR}\left(\infty\right) - \phi_{0}, \label{eq:time_series:invert_proof:d}
\end{align}
\end{subequations}

\noindent where in \cref{eq:time_series:invert_proof:c} we have used
the sum of a geometric series \cref{eq:additional:misc:math:geometric}.
The proof of \cref{eq:time_series:invert:AR_1} is similar and also relies on the geometric series.
Essentially we are making the recursive nature of these time series explicit and using it to transform from one to the other.
The $\text{AR}\left(\infty\right)$ expansion of $\text{MA}\left(1\right)$
is particularly useful as it is possible in practice to truncate the series at $\order{\phi^{p}}$
where $\phi^{p+1} \approx 0$ and represent $\text{MA}\left(1\right)$ as a pure function of the series itself, \ie no recursion.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{ARMA Model}
\label{additional:time_series:ARMA}
% TODO

$\text{ARMA}\left(p,q\right)$

% TODO include lag polynomial definition
% TODO include python function

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Integrated (I) Models}
\label{additional:time_series:I}
% TODO

$\text{I}\left(d\right)$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{ARIMA Model}
\label{additional:time_series:ARIMA}
% TODO

$\text{ARIMA}\left(p,d,q\right)$

% TODO include python function
% TODO include example plots and a link to the example notebook
% TODO talk about how fit is actually computed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Seasonal ARIMA Model (SARIMA)}
\label{additional:time_series:SARIMA}
% TODO

% TODO include python function

% TODo sections for the Box--Jenkins method, ARCH, GARCH, VAR, Anomaly Detection? At least mention them...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluating Time Series Models}
\label{additional:time_series:eval}

Like other machine learning problems, we must split our data into train and test sets
when fitting and evaluating time series models.
As we are extrapolating we use the first
$\approx \SI{70}{\percent}$ of the data by time for the training set,
and the reminding $\approx \SI{30}{\percent}$ for the test set,
instead of randomly splitting the data as is done for interpolative models.
To improve on this approach even further, we can use
``rolling forecast origin'' techniques to evaluate the model.
In short, we use the first $k$ data points to train the model,
evaluate it on the next $k + l$ data points, increment $k$ and reiterate.
Performance metrics can then be averaged across the iterations.
In this way the model's near term predictions at each iteration are as accurate as possible,
as if time had continued and we received more data,
giving a better estimate of the predictive performance in practice.
There are a few different methods for how to exactly roll the origin,
see \cref{fig:rolling_forecast_origin} for details.

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{figures/stats/rolling_forecast_origin}
\caption{
Illustration of multiple test train split methods for time series models \cite{bergmeir_dissertation}.
Note that $t$ is increasing along the $x$-axis.
In the fixed origin method, we do not do any retraining and the prediction quality
can be expected to drop as we go further into the future.
In the rolling origin update method, we do not retrain the model parameters $\bm{\beta}$
with each iteration, but do update the $y_{i}$ lags.
In the rolling origin recalibration method we fully retrain the model on each iteration.
Lastly, in the rolling window method we remove data from the beginning of the training period
as we iterate to keep the training set a constant size.
This can help with some statistical interpretations of the model,
but also helps when working with slowly changing non-stationary data.
}
\label{fig:rolling_forecast_origin}
\end{figure}

Regardless of the test train split method, we can get a sense of the model's performance
qualitatively by looking at residual plots of $y_{t} - \hat{y}_{t}$.
The residuals will show any remaining systematic trends that the model did not learn,
as ideally they would be white noise.
Quantitatively, we can test
if the residuals are white noise with the Ljung--Box test of \cref{additional:time_series:ljung_box},
as well as can compute the
root mean squared error\footnote{Also known as the root mean squared deviation (RMSD).} (RMSE)
and mean absolute percent error (MAPE)
between the predictions and observed values:

\begin{subequations}\label{eq:time_series:RMSE_MAPE}
\begin{align}
\text{RMSE}\left(\hat{y}\right) &= \sqrt{\expval{\left(y - \hat{y}\right)^{2}}} = \sqrt{\frac{1}{n}\sum_{t=1}^{n} \left(y_{t} - \hat{y}_{t}\right)^{2}}, \label{eq:time_series:RMSE} \\
\text{MAPE}\left(\hat{y}\right) &= \frac{\SI{100}{\percent}}{n} \sum_{t=1}^{n} \abs{\frac{y_{t} - \hat{y}_{t}}{y_{t}}}\,. \label{eq:time_series:MAPE}
\end{align}
\end{subequations}

% TODO AIC, BIC

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Unit Roots}
\label{additional:time_series:unit_root}
% TODO
% https://youtu.be/ugOvehrTRRw

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Augmented Dickey--Fuller (ADF) Test for Stationarity}
\label{additional:time_series:ADF}

The augmented\footnote{The ADF is an extension of the normal Dickey--Fuller test where $p = 1$.} Dickey--Fuller (ADF) test
is a hypothesis tests for the stationarity of a time series.
We begin by assuming the time series for $y$ is a $\text{AR}\left(p\right)$ model \cref{eq:time_series:AR}
and construct the difference:

\begin{subequations}\label{eq:time_series:ADF}
\begin{align}
\Delta y_{t} &= y_{t} - y_{t-1} = \phi_{0} + \delta y_{t-1} + \sum_{i=2}^{p} \phi_{i}\, y_{t-i} + \epsilon_{t}, \label{eq:time_series:ADF:Delta_y} \\
\delta &= \phi_{1} - 1. \label{eq:time_series:ADF:delta}
\end{align}
\end{subequations}

Now we test $H_{0}$ that $0 \leq \delta$ and the time series is
non-stationary\footnote{As $\delta = 0 \implies \phi_{1} = 1$, \ie the time series has a unit root and is non-stationary.}
vs the alternative hypothesis $H_{a}$ that $\delta < 0$, \ie time series is stationary.
We fit the data and then perform the hypothesis testing with test statistic
$t_{\hat{\delta}} = \hat{\delta} / \text{Standard Error}\left(\hat{\delta}\right)$
by comparing to the Dickey--Fuller distribution\footnote{We can not use the standard $t$-distribution as $y_{t-1}$ is still assumed to be non-stationary under $H_{0}$.}.
If we can reject $H_{0}$ we therefore have shown that $y_{t}$ is stationary.
Note we can further extend the ADF test by replacing the constant $\phi_{0}$
with a deterministic, quadratic, time trend $\phi_{0} + a_{1} t + a_{2} t^{2}$.
The ADF test is available via the \texttt{statsmodels.tsa.stattools.adfuller}
\href{https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.adfuller.html}{function}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ljung--Box Test for White Noise}
\label{additional:time_series:ljung_box}

The Ljung--Box test tests if a time series $y$ solely consists of white noise
by looking at multiple lags of the ACF.
The test statistic is

\begin{equation}\label{eq:time_series:ljung_box:Q}
Q = n \left(n+2\right) \sum_{i=1}^{h} \frac{\corr{y_{t}}{y_{t-i}}^{2}}{n-i}
\end{equation}

\noindent where $n$ is the sample size and $h$ is the number of lags to be tested.
Using the $\chi^{2}$-distribution with $\nu = h$ degrees of freedom we find the critical $Q_{c}$
and reject $H_{0}$, the data is white noise, if $Q_{c} < Q$.
If we fail to reject $H_{0}$, \ie $Q \leq Q_{c}$, we have shown $y$ is white noise.
Note that if we are performing the Ljung--Box test on the remainders of a fitted model, such as $\text{ARIMA}\left(p,d,q\right)$,
we must adjust the degrees of freedom accordingly, $\nu = h - p - d - q$.
The Ljung--Box test can be performed with the \texttt{statsmodels.stats.diagnostic.acorr\_ljungbox}
\href{https://www.statsmodels.org/dev/generated/statsmodels.stats.diagnostic.acorr_ljungbox.html}{function}.
