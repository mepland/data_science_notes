%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Regression}
\label{chap:regression}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear Regression (OLS)}
\label{regression:linear}

Linear regression fits the best hyperplane, or line in 1D,
to a collection of $m$ points $\vb{x}_{i}, y_{i}$,
typically via the method of least squares.
If $\vb{x}$ has $n$ features we can represent the
linear relationship between $\vb{x}$ and $y$ as:

\begin{equation}\label{eq:linear:one_point}
y_{i} = \beta_{0} + \sum_{j=1}^{n}\, \beta_{j} x_{ij} + \epsilon_{i}\,,
\end{equation}

\noindent where $\beta_{j}$ are the parameters of the regression
and $\epsilon$ represent random errors.
Transitioning to matrix notation\footnote{Note
that \textit{linear} regression refers to the linearity in the model parameters
$\vb*{\beta}$, not $\mb{X}$.
The components of $\mb{X}_{i}$ can be, and often are,
non-linear functions of other input features.}, this is simply:

\begin{equation}\label{eq:linear:matrix}
\vb{y} = \mb{X} \vb*{\beta} + \vb*{\epsilon}\,,
\end{equation}

\noindent where we have set $X_{i,0} = 1$.
Here\footnote{$\mb{X}_{m \times \left(n+1\right)}$,
with the added intercept term,
is known as the regression design matrix.} $\mb{X}$ is $m \times \left(n+1\right)$,
$\vb*{\beta}$ is $\left(n+1\right) \times 1$,
and $\vb{y}$, $\vb*{\epsilon}$ are $m \times 1$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Derivation}
\label{regression:linear:derivation}

The ordinary least squares (OLS) estimate\footnote{When
the errors are assumed to be normal,
as in \cref{item:regression:linear:exogeneity,item:regression:linear:spherical,item:regression:linear:normality},
OLS is equivalent to the maximum likelihood estimation (MLE) approach of \cref{opt:MLE}.
See one derivation \href{https://machinelearningmastery.com/linear-regression-with-maximum-likelihood-estimation/}{here}.} of
the parameters $\hat{\vb*{\beta}}$
can be found by minimizing the squares of the errors,
\ie the objective function $S\left(\vb*{\beta}\right) = \norm{\vb*{\epsilon}}^{2}$:

\begin{subequations} \label{eq:linear:ols}
\begin{align}
\hat{\vb*{\beta}} &= \argmin_{\vb*{\beta}} S\left(\vb*{\beta}\right)\,, \label{eq:linear:argmin} \\
S\left(\vb*{\beta}\right)
&= \norm{\vb{y} - \mb{X} \vb*{\beta}}^{2} = \left(\vb{y} - \mb{X} \vb*{\beta}\right)\transpose \left(\vb{y} - \mb{X} \vb*{\beta}\right) \label{eq:linear:S_matrix} \\
&= \sum_{i=1}^{m} \, \abs{y_{i} - \sum_{j=0}^{n} \, \beta_{j} x_{ij}}^{2}\,. \label{eq:linear:S_components}
\end{align}
\end{subequations}

We then find the minimum of $S\left(\vb*{\beta}\right)$ with respect to $\vb*{\beta}$
by taking the gradient and setting it equal to zero:

\begin{subequations} \label{eq:linear:ols_derivation}
\begin{align}
S\left(\vb*{\beta}\right) &=
 \vb{y}\transpose \vb{y}
-\vb*{\beta}\transpose \mb{X}\transpose \vb{y}
-\vb{y}\transpose \mb{X} \vb*{\beta}
+\vb*{\beta}\transpose \mb{X}\transpose \mb{X} \vb*{\beta} \label{eq:linear:S_expand} \\
&= \vb{y}\transpose \vb{y}
-2\, \vb*{\beta}\transpose \mb{X}\transpose \vb{y}
+\vb*{\beta}\transpose \mb{X}\transpose \mb{X} \vb*{\beta}\,, \label{eq:linear:S_expand_simplified} \\
\partial_{\vb*{\beta}} \, S\left(\vb*{\beta}\right) &=
0
-2\, \partial_{\vb*{\beta}} \, \vb*{\beta}\transpose \mb{X}\transpose \vb{y}
+\partial_{\vb*{\beta}} \, \vb*{\beta}\transpose \mb{X}\transpose \mb{X} \vb*{\beta} \label{eq:linear:grad_S_expand} \\
&= -2\, \mb{X}\transpose \vb{y} + 2\, \mb{X}\transpose \mb{X} \vb*{\beta} = 0. \, \implies \label{eq:linear:grad_S} \\
\mb{X}\transpose \vb{y} &= \mb{X}\transpose \mb{X} \vb*{\beta}\,, \label{eq:linear:penultimate}
\end{align}
\end{subequations}

\noindent where we have used\footnote{See \href{https://economictheoryblog.com/2015/02/19/ols_estimator/}{here}
and \href{https://economictheoryblog.com/2018/10/17/derivation-of-the-least-squares-estimator-for-beta-in-matrix-notation-proof-nr-1/}{here}
for the component-wise proof, but it is essentially the same as the 1D $\partial_{x} b x = b$, $\partial_{x} b x^{2} = 2 b x$.
Also note that \cref{eq:linear:S_expand_simplified} results from $\vb{y}\transpose \mb{X} \vb*{\beta}$ being scalar
and thus equal to its own transpose, $\vb*{\beta}\transpose \mb{X}\transpose \vb{y}$.}:

\begin{subequations} \label{eq:grad_relations}
\begin{align}
\partial_{\vb*{\beta}} \, \vb*{\beta}\transpose \mb{X}\transpose \vb{y} &= \mb{X}\transpose \vb{y}\,, \label{eq:grad_relations:1} \\
\partial_{\vb*{\beta}} \, \vb*{\beta}\transpose \mb{X}\transpose \mb{X} \vb*{\beta} &= 2 \, \mb{X}\transpose \mb{X} \vb*{\beta}\,. \label{eq:grad_relations:2}
\end{align}
\end{subequations}

Consequently, the optimal $\hat{\vb*{\beta}}$ of \cref{eq:linear:ols}
has a closed form solution\footnote{The fitted prediction is $\hat{\vb{y}} = \mb{X} \hat{\vb*{\beta}}$, with residuals $\hat{\vb*{\epsilon}} = \vb{y} - \hat{\vb{y}}$.}:

\begin{equation}\label{eq:linear:betahat_OLS}
\hat{\vb*{\beta}}^{\text{OLS}} = \left(\mb{X}\transpose\mb{X}\right)^{-1}\mb{X}\transpose \vb{y}\,,
\end{equation}

\noindent which is the best linear unbiased estimator (BLUE),
as can be shown with the Gauss-Markov Theorem provided
the following assumptions hold\footnote{For additional details see
\href{https://economictheoryblog.com/2015/04/01/ols_assumptions/}{here},
\href{http://people.duke.edu/~rnau/testing.htm}{here}, and
\href{https://economictheoryblog.com/2015/02/26/markov_theorem/}{here}.}:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Assumptions}
\label{regression:linear:assumptions}

\begin{enumerate}[noitemsep]
  \item The underlying relationship between $\vb{x}$ and $y$ is linear, and there are no major outliers.\label{item:regression:linear:linear}
  \item The columns of $\mb{X}$, \ie features, are linearly independent, \ie full $\rank\left(\mb{X}\right) = n+1$ (no multicollinearity). This allows $\mb{X}\transpose\mb{X}$ to be inverted.\label{item:regression:linear:multicollinearity}
  \item The errors $\epsilon$ have conditional mean 0, $\expvalE{\epsilon \mid \mb{X}} = 0$ (exogeneity). The errors thus:\label{item:regression:linear:exogeneity}
  \begin{enumerate}[noitemsep]
    \item Have a mean of zero, $\expvalE{\epsilon} = 0$.
    \item Are not correlated with the input features, $\expvalE{\mb{X}\transpose\epsilon} = 0$.
  \end{enumerate}
  \item The errors are spherical, $\mathrm{var}\left(\epsilon \mid \mb{X}\right) = \sigma^{2} \identity$. Thus:\label{item:regression:linear:spherical}
  \begin{enumerate}[noitemsep]
    \item Each observation $\vb{x}_{i}$ has the same constant variance $\sigma^{2}$ (homoscedasticity).
    \item The errors are uncorrelated between observations, $\expvalE{\epsilon_{i}\epsilon_{j \neq i} \mid \mb{X}} = 0$ (no autocorrelation).
  \end{enumerate}
  \item The errors are normally distributed (multivariate normality)\footnote{This is not required for OLS to be the BLUE, but hypothesis testing works if true.}.\label{item:regression:linear:normality}
\end{enumerate}

If these assumptions are violated the following issues arise,
namely the model may be biased and or have a large or invalid estimated variance:

\begin{itemize}[noitemsep]
  \item[\cref{item:regression:linear:linear}.] If you are fitting nonlinear data the predictions will have large errors,
particularly when extrapolated beyond the range of the fitted data.
This will show up as systematic errors in the residuals plot,
or may be obvious when comparing observed versus predicted values.
Possible fixes include applying a nonlinear transformation to some of the features to linearize the data, \eg take the log,
adding more combinations of features, \eg higher polynomial terms,
or finding new independent features which may explain the nonlinearity.

  \item[\cref{item:regression:linear:multicollinearity}.] If some of the features are not linearly independent (multicollinearity),
they can bias the model and should be removed in turn until linear independence is restored.
Multicollinearity can be spotted in the input feature correlation matrix,
with the variance inflation factor (VIF) of \cref{regression:VIF},
or if the residuals correlate to any of the features.
Multicollinearity reduces the interpretability of a regression model's coefficients,
as changes in any of the multicollinear features could be the reason for a change in the model's output,
making it difficult to disentangle the effect of any particular feature.

  \item[\cref{item:regression:linear:exogeneity}.] If something is wrong with the errors
such that they have a non-zero mean or correlate to the input features
the OLS $\hat{\vb*{\beta}}$ is biased and inconsistent.
This can happen if there are omitted variables or measurement errors.
Furthermore, if $\expvalE{\epsilon \mid \mb{X}} = c$, only the $\beta_{0}$ intercept is affected.

  \item[\cref{item:regression:linear:spherical}.] If something is wrong with the errors
such that they have a changing variance or correlate across observations\footnote{Thus the residuals correlate with row number, \ie autocorrelation.}
the reported confidence intervals on the model parameters may be over or underestimated.
The OLS $\hat{\vb*{\beta}}$ remains unbiased and has consistent coefficients, but will be biased for standard errors.

  \item[\cref{item:regression:linear:normality}.] If the errors are not normally distributed the confidence intervals are again suspect.
This can be diagnosed by comparing the errors to the normal distribution with a normal probability plot, or normal quantile plot,
or through a statistical method like the Anderson-Darling and Kolmogorov-Smirnov tests.
Note that violating normality in the errors is not as much of an issue compared to the other assumptions,
as the fit will still give usable coefficients provided the assumed form of the model is correct.
Problems of this kind can arise from nonlinear data or influential outliers.
If the errors really are non-normal, a generalized linear model (GLM) could be employed to model them correctly.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ridge Regression (L2, or Tikhonov)}
\label{regression:linear:ridge}

Adding ridge (L2, or Tikhonov) regularization to OLS is much more straight forward than LASSO,
so we shall discuss it first\footnote{See \href{https://stats.stackexchange.com/a/164546}{here} for an interesting geometric explanation.}.
We wish to minimize the square errors $\norm{\vb*{\epsilon}}^{2}$,
now subject to the condition that $\norm{\vb*{\beta}}^{2} < t$ for some $t \geq 0$.
For any $t \geq 0$ there is a $\lambda \geq 0$ such that minimizing the following
objective function $S\left(\vb*{\beta}\right)$ without conditions is
equivalent\footnote{This is sometimes termed soft-thresholding, but is really an application of Lagrange multipliers, as described in \cref{opt:lagrange_mult}.}:

\begin{equation} \label{eq:linear:ridge}
S\left(\vb*{\beta}\right) = \norm{\vb{y} - \mb{X} \vb*{\beta}}^{2} + \lambda \norm{\vb*{\beta}}^{2}\,.
\end{equation}

After taking the gradient, we have the same terms as \cref{eq:linear:grad_S},
plus a new term\footnote{Note the similarity to \cref{eq:grad_relations:2}.} \cref{eq:linear:ridge_derivation:new}.
Rearranging \cref{eq:linear:ridge_derivation:penultimate} we can move the new term in with the old,
and quickly arrive at a modified version of the OLS solution \cref{eq:linear:ridge_derivation:betahat}.

\begin{subequations} \label{eq:linear:ridge_derivation}
\begin{align}
\partial_{\vb*{\beta}} \, S\left(\vb*{\beta}\right)
&= -2\, \mb{X}\transpose \vb{y} + 2\, \mb{X}\transpose \mb{X} \vb*{\beta} \label{eq:linear:ridge_derivation:grad_S_repeat} \\
&+ 2\,\lambda \vb*{\beta} = 0 \,\, \implies \label{eq:linear:ridge_derivation:new} \\
\mb{X}\transpose \vb{y} &= \left(\mb{X}\transpose \mb{X} + \lambda\,\identity\right) \vb*{\beta} \label{eq:linear:ridge_derivation:penultimate} \\
\hat{\vb*{\beta}}^{\text{ridge}} &= \left(\mb{X}\transpose\mb{X} + \lambda\,\identity\right)^{-1}\mb{X}\transpose \vb{y} \label{eq:linear:ridge_derivation:betahat}
\end{align}
\end{subequations}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LASSO Regression (L1)}
\label{regression:linear:lasso}

As promised, adding LASSO (L1) regularization to OLS is challenging
as the condition, $\norm{\vb*{\beta}} < t$ for some $t \geq 0$,
is not differentiable, see the corners of \cref{fig:ml:l1l2:l1}, and must be dealt with carefully.
In the special case that $\mb{X}$ is orthonormal, $\mb{X}\transpose \mb{X} = \identity$,
a closed form solution \cref{eq:linear:lasso:betahat:1} can be derived on a case-by-case basis
at the $\beta_{j}$ coordinate-level\footnote{See \href{https://stats.stackexchange.com/questions/17781/derivation-of-closed-form-lasso-solution}{here},
\href{https://en.wikipedia.org/wiki/Lasso_(statistics)\#Orthonormal_covariates}{here},
and \href{https://xavierbourretsicotte.github.io/lasso_derivation.html}{here}.}.
In the general case, more sophisticated methods can find numerical solutions.

\begin{subequations} \label{eq:linear:lasso:betahat_all}
\begin{align}
\hat{\vb*{\beta}}^{\text{LASSO}}_{j}
&= \hat{\vb*{\beta}}^{\text{OLS}}_{j} \max\left(0, \frac{\lambda}{\abs{\hat{\vb*{\beta}}^{\text{OLS}}_{j}}}\right) \label{eq:linear:lasso:betahat:1} \\
&= \sign{\hat{\vb*{\beta}}^{\text{OLS}}_{j}} \left(\abs{\hat{\vb*{\beta}}^{\text{OLS}}_{j}} - \lambda\right)^{+} \label{eq:linear:lasso:betahat:2} \\
\hat{\vb*{\beta}}^{\text{OLS}}
&= \left(\mb{X}\transpose\mb{X}\right)^{-1}\mb{X}\transpose \vb{y}
= \mb{X}\transpose \vb{y} \label{eq:lasso:modified_betahat_OLS}
\end{align}
\end{subequations}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LASSO versus Ridge Regression}
\label{regression:linear:lasso_vs_ridge}

All the usual concepts from \cref{ml_general:reg} apply, but specifically for linear regression
both LASSO and ridge allow us to relax the multicollinearity condition.
LASSO tends to use one feature per group of correlated $\vb{x}_{j}$'s, setting the rest to $\approx 0$,
while ridge will keep all of the associated $\beta_{j}$'s at a similar magnitude.
As one may expect from LASSO shrinking $\beta_{j}$'s to zero,
it does a better job when only some of the $n$ input features influence \yhat,
while ridge does better if most of the features have similar importances.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Variance of Coefficients}
\label{regression:linear:coeff_variance}

The variance of the $\beta_{j}$ coefficient
is the $j+1$, $j+1$ element of $s^{2}\left(\mb{X}\transpose \mb{X}\right)^{-1}$,
where $\mb{X}$ is the regression design
matrix\footnote{$X_{i,0} = 1$ for the intercept term, and $X_{i,j+1} = x_{i,j}$ otherwise, as in \cref{regression:linear}.} and
$s$ is the standard error of the regression \cref{eq:regression:goodness_of_fit:s}.
A simpler form\footnote{See \href{https://en.wikipedia.org/wiki/Variance_inflation_factor\#Definition}{here} for a derivation.} \cref{eq:regression:linear:coeff_variance}
shows the dependence on
$s$, \ie goodness of fit,
sample size $m$,
$\variance{x_{j}}$, \ie variance in the $\vb{x}_{j}$ feature,
and the variance inflation factor (VIF) of \cref{regression:VIF}, \ie a measure of $\vb{x}_{j}$'s multicollinearity.
Note that a larger $\variance{x_{j}}$ will actually decrease \variance{\beta_{j}},
as the hyperplane is more constrained in $\vu{e}_{j}$ when the $x_{j}$ values are non-degenerate.

\begin{equation}\label{eq:regression:linear:coeff_variance}
\variance{\beta_{j}} = \frac{s^{2}}{\left(m-1\right)\variance{x_{j}}} \, \text{VIF}_{j}\,.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Coefficient Significance}
\label{regression:linear:coeff_significance}
% TODO
% What is the significance of any particular $\beta_{j}$ coefficient, via a \ttest, \chiSqtest, \Ftest?
% Should be able to get the distribution of any $\beta_{j}$ analytically for OLS, but can use bootstrap sampling for more complex models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{OLS Example}
\label{regression:linear:example}
% TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Goodness of Fit}
\label{regression:goodness_of_fit}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Coefficient of Determination (\texorpdfstring{$R^{2}$}{R-Squared})}
\label{regression:goodness_of_fit:R2}

The coefficient of determination\footnote{For additional details see
\href{https://economictheoryblog.com/2014/11/05/the-coefficient-of-determination-latex-r2/}{here},
\href{https://economictheoryblog.com/2014/11/05/proof/}{here}, and
\href{http://people.duke.edu/~rnau/rsquared.htm}{here}.}, $R^{2}$,
measures how much of $\vb{y}$'s variance is explained by the fitted model $\hat{\vb{y}}$,
versus by random errors $\vb*{\epsilon}$ or other unknown sources,
as $\vb{y} = \hat{\vb{y}} + \vb*{\epsilon}$.
$R^{2} \approx 1$ is ideal and, depending on the particular definition,
it can take values outside the typical $0 \leq R^{2} \leq 1$ range.

\begin{equation}\label{eq:regression:goodness_of_fit:R2}
R^{2} = \frac{
\sum_{i=1}^{m} \left(\yhat_{i} - \expval{\yhat}\right)^{2}
}{
\sum_{i=1}^{m} \left(y_{i} - \expval{y}\right)^{2}
} = 1 - \frac{
\sum_{i=1}^{m} \hat{\epsilon}_{i}^{\,2}
}{
\sum_{i=1}^{m} \left(y_{i} - \expval{y}\right)^{2}
} = \rho_{y,\yhat}^{2}
\end{equation}

The adjusted $R^{2}$, $R^{2}_{\text{adj}}$, accounts for the degrees of freedom in the
fitted sample, $m-1$, and in the model, $m-n-1$, and is thus a better metric.

\begin{equation}\label{eq:regression:goodness_of_fit:R2_adj}
R^{2}_{\text{adj}} = 1 - \frac{
\left(\sum_{i=1}^{m} \hat{\epsilon}_{i}^{\,2}\right)/\left(m-n-1\right)
}{
\left(\sum_{i=1}^{m} \left(y_{i} - \expval{y}\right)^{2}\right)/\left(m-1\right)
}
= 1 - \left(1-R^{2}\right)\frac{m-1}{m-n-1}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Standard Error of the Regression and \texorpdfstring{$\chi_{\nu}^{2}$}{Chi-Nu}}
\label{regression:goodness_of_fit:reduced_chi2}

The standard error of the regression, $s$,
and reduced \chiSqstat, $\chi_{\nu}^{2}$,
are additional ways of quantifying goodness of fit.
$s$ is a measure of the typical residual, in the units of $y$.
$\chi_{\nu}^{2}$ is a convenient way of reporting this spread in a dimensionless manner,
particularly in more sophisticated regressions where each $y_{i}$ has an \apriori estimated uncertainty, $\sigma_{i}$.

\begin{subequations}\label{eq:regression:goodness_of_fit:s_red_chi2}
\begin{align}
s^{2} &= \frac{\norm{\hat{\vb*{\epsilon}}}^{2}}{m-n} = \frac{1}{m-n} \sum_{i=1}^{m} \left(\yhat_{i} - y_{i}\right)^{2} \label{eq:regression:goodness_of_fit:s} \\
\chi_{\nu}^{2} &= \frac{1}{m-n} \sum_{i=1}^{m} \frac{\left(\yhat_{i} - y_{i}\right)^{2}}{\sigma_{i}^{2}} \label{eq:regression:goodness_of_fit:red_chi2}
\end{align}
\end{subequations}

$\chi_{\nu}^{2}$ can be interpreted as follows:

\begin{table}[H]
  \centering
  \begin{tabular}{c | p{8cm}}
$\chi_{\nu}^{2} \gg 1$ & The true model may be different than the fitted model. \\
$\chi_{\nu}^{2} > 1$ & Fit doesn't fully explain data (underfitting), or the \apriori $\sigma_{i}$ are underestimated. \\
$\chi_{\nu}^{2} \approx 1$ & Good fit! \\
$\chi_{\nu}^{2} < 1$ & Fit overexplains data (overfitting), or the \apriori $\sigma_{i}$ are overestimated. \\
$\chi_{\nu}^{2} \ll 1$ & Extreme overfitting or overestimated $\sigma_{i}$.
  \end{tabular}
%  \caption{}
  \label{table:red_chi2_interp}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\texorpdfstring{$F$}{F}-Test of Lack-of-Fit Sum of Squares}
\label{regression:goodness_of_fit:F_test_fit}
% TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Variance Inflation Factor (VIF)}
\label{regression:VIF}

The variance inflation factor (VIF) is a metric for quantifying
the degree of multicollinearity present in a given feature $\vb{x}_{j}$
with respect to the other $n-1$ features of a dataset.
The VIF is computed by regressing $\vb{x}_{j}$ against the other features,
$x_{ij} = \beta_{0} + \sum_{j' \neq j}\, \beta_{j'} x_{ij'} + \epsilon_{i}$,
then taking the resulting coefficient of determination $R^{2}_{j}$ \cref{eq:regression:goodness_of_fit:R2}
and transforming it as:

\begin{equation}\label{eq:regression:VIF}
\text{VIF}_{j} = \frac{1}{1 - R_{j}^{2}}.
\end{equation}

\noindent Note, we need to perform a new regression for each $\vb{x}_{j}$;
these regressions are entirely separate from the main regression against $\vb{y}$.
The \texttt{variance\_inflation\_factor}
\href{https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html}{function}
of \texttt{statsmodels} in \python
streamlines this process and returns $\text{VIF}_{j}$ for a given $\mb{X}$ and $j$.
Some software packages may return the tolerance instead of the VIF,
but this is just the inverse of the VIF.

The $\text{VIF}_{j}$ can be interpreted as
a multiplicative factor on the variance of $\beta_{j}$
that would have been observed in the absence of multicollinearity,
\ie $\variance{\beta_{j}} = \text{VIF}_{j} \, \variance{\beta_{j}}_{\text{no collinearity}}$.
Commonly used guidelines for VIF values are:

\begin{table}[H]
  \centering
  \begin{tabular}{c | p{5cm}}
$10 < \text{VIF}$ & Strong multicollinearity! \\
$5 < \text{VIF}$ & Concerning multicollinearity \\
$ 1 < \text{VIF} < 5$ & Moderate multicollinearity \\
$\text{VIF} = 1$ & No multicollinearity present
  \end{tabular}
  \caption{Interpretation guidelines for VIF values \cite{Kutner2004,Sheather2009}.}
  \label{table:VIF_guidelines}
\end{table}

If $\vb{x}_{j}$ has $5 \lesssim \text{VIF}_{j}$,
the analyst should review $\mb{X}$ using any relevant subject matter knowledge
to determine if a known, or suspected, collinearity, \ie correlation,
maybe present between $\vb{x}_{j}$ and one or more features.
Note, a high $\text{VIF}_{j}$ only indicates that $\vb{x}_{j}$ has a multicollinearity issue,
not which of the other $n-1$ features are the problem.
Indeed, there can be multiple disjoint sets of interrelated features all with high VIFs
that need to be sorted out empirically.
Probing $\text{VIF}_{j}$ while leaving some $\vb{x}_{j' \neq j}$ out of $\mb{X}$ can help here.
Possible resolutions to multicollinearity issues include
engineering new non-collinear features and feature removal.
However, multicollinearity problems can be tolerated
if the model coefficients $\beta_{j}$ do not need to be interpretable,
as multicollinearity only affects the relative values of $\beta_{j}$
and not the quality of the output \yhat.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bayesian Linear Regression}
\label{regression:bayesian_linear}
% TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LASSO and Ridge as Priors}
\label{regression:bayesian_linear:lasso_vs_ridge}
% TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Weighted Least Squares}
\label{regression:WLS}
% TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generalized Least Squares (GLS)}
\label{regression:GLS}
% TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generalized Linear Models (GLM)}
\label{regression:GLM}
% TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Binomial Regression}
\label{regression:GLM:binomial}
% TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Poisson Regression}
\label{regression:GLM:poisson}
% TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gaussian Process Regression (Kriging)}
\label{regression:kriging}
% TODO

% TODO see \cite{Brochu2010} section 2.6 and the additional references it cites

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fixed-Effects Model}
\label{regression:fixed_effects}
% TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Mixed-Effects Model}
\label{regression:mixed_effects}
% TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Simpson's Paradox}
\label{regression:simpsons_paradox}
% TODO
